# Pro-Mata Complete Stack - Docker Swarm Multi-Node
# Configuração para deployment em N máquinas (1 manager + N workers)

version: "3.8"

services:
  # Database Migration Init Container
  db-migrate:
    image: norohim/pro-mata-migration:${MIGRATE_VERSION:-latest}
    environment:
      - NODE_ENV=${NODE_ENV:-development}
      - DATABASE_URL=postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres-primary:5432/${POSTGRES_DB}
      - DB_HOST=postgres-primary
      - DB_PORT=5432
      - POSTGRES_USER=${POSTGRES_USER}
      - RUN_SEED=${RUN_SEED:-false}
    networks:
      - database_tier
    depends_on:
      - postgres-primary
    deploy:
      replicas: 1
      placement:
        constraints: [node.role == manager]
      restart_policy:
        condition: none  # Execute apenas uma vez
      resources:
        limits:
          memory: 256M
        reservations:
          memory: 128M

  # PostgreSQL Primary (Master)
  postgres-primary:
    image: promata/database:${DB_VERSION:-latest}
    environment:
      - POSTGRES_DB=${POSTGRES_DB}
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_REPLICATION_USER=${POSTGRES_REPLICA_USER:-replicator}
      - POSTGRES_REPLICATION_PASSWORD=${POSTGRES_REPLICA_PASSWORD}
      - POSTGRES_REPLICATION_MODE=master
      - PGUSER=${POSTGRES_USER}
      
    volumes:
      - postgres_primary_data:/var/lib/postgresql/data
      - postgres_backups:/var/lib/postgresql/backups
    
    networks:
      - database_tier
    
    deploy:
      replicas: 1
      placement:
        constraints: [node.labels.database.primary == true]
      restart_policy:
        condition: on-failure
        delay: 10s
        max_attempts: 3
      resources:
        limits:
          memory: 2G
          cpus: '1.5'
        reservations:
          memory: 1G
          cpus: '0.5'
    
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s

  # PostgreSQL Replica (Standby) - Can scale across workers
  postgres-replica:
    image: promata/database:${DB_VERSION:-latest}
    environment:
      - PGUSER=${POSTGRES_REPLICA_USER:-replicator}
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_DB=${POSTGRES_DB}
      - POSTGRES_PRIMARY_HOST=postgres-primary
      - POSTGRES_REPLICATION_USER=${POSTGRES_REPLICA_USER:-replicator}
      - POSTGRES_REPLICATION_MODE=slave
      - POSTGRES_REPLICATION_PASSWORD=${POSTGRES_REPLICA_PASSWORD}
      
    volumes:
      - postgres_replica_data:/var/lib/postgresql/data
    
    networks:
      - database_tier
    
    depends_on:
      - postgres-primary
    
    deploy:
      replicas: ${POSTGRES_REPLICA_COUNT:-1}
      placement:
        constraints: [node.labels.database.replica == true]
        preferences:
          - spread: node.labels.database.replica
      restart_policy:
        condition: on-failure
        delay: 15s
        max_attempts: 3
      resources:
        limits:
          memory: 1G
          cpus: '1.0'
        reservations:
          memory: 512M
          cpus: '0.25'
    
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_REPLICA_USER:-replicator}"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 45s

  # PgBouncer - Connection Pooling (Scaled across nodes)
  pgbouncer:
    image: pgbouncer/pgbouncer:latest
    environment:
      - DATABASES_HOST=postgres-primary
      - DATABASES_PORT=5432
      - DATABASES_USER=${POSTGRES_USER}
      - DATABASES_PASSWORD=${POSTGRES_PASSWORD}
      - DATABASES_DBNAME=${POSTGRES_DB}
      - POOL_MODE=${PGBOUNCER_POOL_MODE:-transaction}
      - MAX_CLIENT_CONN=${PGBOUNCER_MAX_CLIENT_CONN:-200}
      - DEFAULT_POOL_SIZE=${PGBOUNCER_POOL_SIZE:-25}
      - ADMIN_USERS=${POSTGRES_USER}
      - STATS_USERS=${POSTGRES_USER}
    
    networks:
      - database_tier
      - app_tier
    
    depends_on:
      - postgres-primary
      - db-migrate
    
    deploy:
      replicas: ${PGBOUNCER_REPLICAS:-3}
      placement:
        preferences:
          - spread: node.id
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
      resources:
        limits:
          memory: 256M
          cpus: '0.5'
        reservations:
          memory: 128M
          cpus: '0.1'
    
    healthcheck:
      test: ["CMD-SHELL", "psql -h localhost -p 6432 -U ${POSTGRES_USER} -d pgbouncer -c 'SHOW STATS'"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s

  # Backend Service - Horizontally scaled across nodes
  backend:
    image: ${BACKEND_IMAGE}
    environment:
      - NODE_ENV=production
      - PORT=3000
      - DATABASE_URL=postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@pgbouncer:6432/${POSTGRES_DB}
      - JWT_SECRET=${JWT_SECRET}
      - JWT_EXPIRES_IN=${JWT_EXPIRES_IN:-1h}
      - CORS_ORIGIN=https://${DOMAIN_NAME}
      - DB_HOST=pgbouncer
      - DB_PORT=6432
    
    networks:
      - app_tier
      - proxy_tier
    
    depends_on:
      - pgbouncer
    
    deploy:
      replicas: ${BACKEND_REPLICAS:-5}
      placement:
        preferences:
          - spread: node.id
        constraints: 
          - node.role != manager  # Evita sobrecarregar o manager
      update_config:
        parallelism: 1
        delay: 15s
        order: start-first
        failure_action: rollback
      rollback_config:
        parallelism: 1
        delay: 10s
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.1'
    
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # Frontend Service - Distributed across nodes  
  frontend:
    image: ${FRONTEND_IMAGE}
    environment:
      - NODE_ENV=production
      - VITE_API_URL=https://api.${DOMAIN_NAME}
      - VITE_APP_ENV=${ENVIRONMENT}
      - VITE_APP_VERSION=${FRONTEND_VERSION:-latest}
    
    networks:
      - proxy_tier
    
    deploy:
      replicas: ${FRONTEND_REPLICAS:-4}
      placement:
        preferences:
          - spread: node.id
        constraints:
          - node.role != manager
      update_config:
        parallelism: 2
        delay: 10s
        order: start-first
      resources:
        limits:
          memory: 256M
          cpus: '0.25'
        reservations:
          memory: 128M
          cpus: '0.1'
    
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s

  # Traefik Load Balancer - Manager node
  traefik:
    image: traefik:v3.0
    command:
      - "--api.dashboard=true"
      - "--api.insecure=false"
      - "--providers.docker=true"
      - "--providers.docker.swarmmode=true"
      - "--providers.docker.exposedbydefault=false"
      - "--entrypoints.web.address=:80"
      - "--entrypoints.websecure.address=:443"
      - "--certificatesresolvers.letsencrypt.acme.tlschallenge=true"
      - "--certificatesresolvers.letsencrypt.acme.email=${LETSENCRYPT_EMAIL}"
      - "--certificatesresolvers.letsencrypt.acme.storage=/letsencrypt/acme.json"
      - "--log.level=INFO"
      - "--accesslog=true"
      - "--metrics.prometheus=true"
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - traefik_letsencrypt:/letsencrypt
    networks:
      - proxy_tier
    deploy:
      replicas: 1
      placement:
        constraints: [node.role == manager]
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.1'
      labels:
        # Traefik Dashboard
        - traefik.enable=true
        - traefik.docker.network=proxy_tier
        - traefik.http.routers.traefik.rule=Host(`traefik.${DOMAIN_NAME}`)
        - traefik.http.routers.traefik.entrypoints=websecure
        - traefik.http.routers.traefik.tls.certresolver=letsencrypt
        - traefik.http.routers.traefik.service=api@internal
        - traefik.http.routers.traefik.middlewares=traefik-auth
        - traefik.http.middlewares.traefik-auth.basicauth.users=${TRAEFIK_AUTH_USERS}

  # Redis Cache - Distributed across workers
  redis:
    image: redis:7-alpine
    command: redis-server --appendonly yes --maxmemory 256mb --maxmemory-policy allkeys-lru
    volumes:
      - redis_data:/data
    networks:
      - app_tier
    deploy:
      replicas: ${REDIS_REPLICAS:-2}
      placement:
        preferences:
          - spread: node.id
        constraints:
          - node.role == worker
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.1'

  # Monitoring Stack - Prometheus
  prometheus:
    image: prom/prometheus:latest
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=200h'
      - '--web.enable-lifecycle'
    volumes:
      - prometheus_data:/prometheus
      - prometheus_config:/etc/prometheus
    networks:
      - monitoring_tier
      - app_tier
    deploy:
      replicas: 1
      placement:
        constraints: [node.role == manager]
      resources:
        limits:
          memory: 1G
          cpus: '0.5'
        reservations:
          memory: 512M
          cpus: '0.1'

  # Grafana Dashboard
  grafana:
    image: grafana/grafana:latest
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD}
      - GF_INSTALL_PLUGINS=grafana-piechart-panel
    volumes:
      - grafana_data:/var/lib/grafana
    networks:
      - monitoring_tier
      - proxy_tier
    deploy:
      replicas: 1
      placement:
        constraints: [node.role == manager]
      resources:
        limits:
          memory: 512M
          cpus: '0.25'
        reservations:
          memory: 256M
          cpus: '0.1'
      labels:
        - traefik.enable=true
        - traefik.docker.network=proxy_tier
        - traefik.http.routers.grafana.rule=Host(`grafana.${DOMAIN_NAME}`)
        - traefik.http.routers.grafana.entrypoints=websecure
        - traefik.http.routers.grafana.tls.certresolver=letsencrypt
        - traefik.http.services.grafana.loadbalancer.server.port=3000

networks:
  database_tier:
    driver: overlay
    internal: true
    attachable: false
  app_tier:
    driver: overlay
    internal: true  
    attachable: false
  proxy_tier:
    driver: overlay
    attachable: false
  monitoring_tier:
    driver: overlay
    internal: true
    attachable: false

volumes:
  postgres_primary_data:
    driver: local
  postgres_replica_data:
    driver: local
  postgres_backups:
    driver: local
  redis_data:
    driver: local
  traefik_letsencrypt:
    driver: local
  prometheus_data:
    driver: local
  prometheus_config:
    driver: local
  grafana_data:
    driver: local